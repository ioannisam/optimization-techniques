\documentclass[a4paper,12pt]{article}

\usepackage{fontspec}
\usepackage{microtype}
\usepackage{polyglossia}

\setmainlanguage{greek}
\setotherlanguage{english}

\setmainfont{Latin Modern Roman}
\newfontfamily\greekfont{CMU Serif}[Script=Greek]

\usepackage[a4paper, top=1.5cm, bottom=1.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{array}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    urlcolor=blue
}
\usepackage[super,sort&compress]{cite}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{csvsimple}
\usepackage{tabularx}
\usepackage{makecell}

\newcolumntype{C}{>{\centering\arraybackslash}X}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}
\newcolumntype{M}{>{\centering\arraybackslash}p{2.5cm}}

\title{Τεχνικές Βελτιστοποίησης: Εργασία 2}
\author{Ιωάννης Μιχάλαινας ΑΕΜ:10902}
\date{Νοέμβριος 2025}

\begin{document}

\maketitle

\begin{center}
\textbf{Αποθετήριο Κώδικα:} \\
\href{https://github.com/ioannisam/optimization-techniques}
     {github.com/ioannisam/optimization-techniques}
\end{center}

\bigskip

\begin{abstract}
    Στην εργασία αυτή υλοποιούνται και συγκρίνονται n-διάστατες μέθοδοι βελτιστοποίησης - συγκεκριμένα η μέθοδος Μέγιστης Καθόδου, η μέθοδος Newton και η μέθοδος Levenberg-Marquardt. Για καθεμία από αυτές εξετάζεται ο αριθμός υπολογισμών και η σύγκλιση, ως προς την ταχύτητα και την ακρίβεια, εφαρμόζοντάς τες από διαφορετικά αρχικά σημεία και με διαφορετικές μεθόδους υπολογισμού του βήματος. Τα αποτελέσματα δείχνουν τις ομοιότητες και τις διαφορές μεταξύ των μεθόδων και αναδεικνύουν τα πλεονεκτήματα και τους περιορισμούς της καθεμιάς. Ο σκοπός της εργασίας αυτής είναι η εξοικείωση με τις παραπάνω μεθόδους και η σύγκριση μεταξύ τους, όσον αφορά την αποδοτικότητα.
\end{abstract}

\tableofcontents
\clearpage

\section{Εισαγωγή}

Στην εργασία αυτή μελετάμε διάφορες τεχνικές βελτιστοποίησης, ονομαστικά τη μέθοδο Μέγιστης Καθόδου, τη μέθοδο Newton και τη μέθοδο Levenberg-Marquardt. Ο σκοπός είναι η προσέγγιση του $x^*$, της λύσης του προβλήματος ελαχιστοποίησης.

Οι μέθοδοι που θα εξετάσουμε βασίζονται στην ιδέα της επαναληπτικής καθόδου, η οποία οδηγεί σε ολοένα και βελτιωμένες τιμές της $f$. $$f(x_{k+1}) < f(x_k), \quad k=0,1,2,\dots$$

Στόχος των μεθόδων αυτών είναι με κάθε επανάληψη συγκλίνουμε προς το $x^*$. Αυτό επιτυγχάνεται μέσω της επαναληπτικής σχέσης: $$x_{k+1}=x_k+\gamma_kd_k$$ Όπου $k$ ο αριθμός των επαναλήψεων, $d_k$ το διάνυσμα της κατεύθυνσης του αλγορίθμου που ικανοποιεί τη συνθήκη $\nabla f^\top(x_k)d_k<0$ και $\gamma_k>0$, τέτοιο ώστε να ισχύει η συνθήκη $f(x_k+\gamma_kd_k)<f(x_k)$.

Οι αλγόριθμοι που θα μελετήσουμε είναι της γενικής μορφής $$x_{k+1}=x_k-\gamma_k\Delta_k \nabla f(x_k)$$

Η επιλογή του βήματος $\gamma_k$ είναι νευραλγικής σημασίας για την ορθότητα του αλγορίθμου, αφού αν επιλεγεί πολύ μικρό θα οδηγήσει σε ασήμαντη μείωση της $f$ και κατά συνέπεια αργή σύγκλιση. Αντίθετα, αν επιλεγεί πολύ μεγάλο μπορεί να οδηγήσει τον αλγόριθμο σε αστάθεια. Θα εξετάσουμε τρεις διαφορετικούς τρόπους προσδιορισμού του βήματος:
\begin{itemize}
    \item \textbf{Σταθερό Βήμα:} Κρατάμε σταθερό το βήμα $\gamma_k$ καθ' όλη τη διάρκεια της αναζήτησης. Αποτελεί την απλούστερη μέθοδο, αφού ελαχιστοποιεί την υπολογιστική πολυπλοκότητα του αλγορίθμου. Δεδομένης της ευαισθησίας του ζητήματος της επιλογής βήματος, αυτή η μέθοδος προϋποθέτει μεγάλη εμπειρεία και ακρίβεια για την ορθή εφαρμογή της.
    \item \textbf{Βέλτιστο Βήμα:} Επιλέγουμε βήμα τέτοιο ώστε να ελαχιστοποιηθεί η $f(x_k+\gamma_k d_k)$ ως προς $\gamma_k$. Αυτό αποτελεί μονοδιάστατο πρόβλημα βελτιστοποίησης που σε αυτή την εργασία θα επιλύεται με τη βοήθεια της μεθόδου Χρυσής Τομής, υλοποιημένης στην προηγούμενη εργασία. Επιλέγουμε τη μέθοδο Χρυσής Τομής διότι είναι η ταχύτερη μέθοδος χωρίς χρήση παραγώγων που δεν απαιτεί τον εκ των προτέρων υπολογισμό των συνολικών επαναλήψεων.
    \item \textbf{Κανόνας Armijo:} Σύμφωνα με τον κανόνα Armijo, το βήμα επιλέγεται ως $\gamma_k=s\beta^{\mu_k}$, όπου $\mu_k \in \mathbb{Z}_{\ge 0}$ τέτοιο ώστε $f(x_k)-f(x_{k+1}) \ge -\alpha \beta^{\mu_k}sd_k^{\top} \nabla f(x_k)$. Εδώ ο συντελεστής s δηλώνει το αρχικό βήμα. Συνήθως ισχύει $\alpha \in [10^{-5}, 10^{-1}]$ και $\beta \in [{1\over10}, {1\over2}]$.
\end{itemize}


Δοσμένων των αρχικών σημείων $[0,0]$, $[-1,-1]$ και $[1,1]$ θα ελαχιστοποιηθεί η: $$f(x,y) = x^3e^{-x^2-y^4}$$

Η συνάρτηση $f$ είναι δύο φορές παραγωγίσιμη, συνθήκη απαραίτητη για την εφαρμογή των αλγορίθμων που θα εξετάσουμε. Παρακάτω παρουσιάζεται η γραφική της παράσταση τόσο στον τρισδιάστατο χώρο όσο και στον δισδιάστατο χώρο, υπό τη μορφή ισοβαρών καμπυλών. Αυτή φαίνεται να παρουσιάζει ελάχιστο στο $[-1.225, 0.009]$ με τιμή $f_{min}=-0.41$. Για θετικές τιμές του $x$, η συνάρτηση αποκτά θετικές τιμές και σχηματίζει έναν ομαλό λόφο, ενώ για αρνητικές τιμές του $x$ εμφανίζεται μια βαθιά κοιλότητα η οποία οδηγεί στο τοπικό ελάχιστο που εντοπίστηκε.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/task1_1.jpg}
    \caption{Τρισδιάστατη απεικόνιση της συνάρτησης f}
    \label{fig:task1_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/task1_2.jpg}
    \caption{Ισοεπίπεδη απεικόνιση της συνάρτησης f}
    \label{fig:task1_2}
\end{figure}

\newpage
\section{Μέθοδος Μέγιστης Καθόδου}

    \subsection{Αλγόριθμος}

    Η μέθοδος της Μέγιστης Καθόδου προκύπτει επιλέγοντας $\Delta_k=I, \quad k=1,2,\dots$ και η επαναληπτική σχέση, λοιπόν, γίνεται: $$x_{k+1}=x_k-\gamma_k \nabla f(x_k)$$
    Το διάνυσμα κατεύθυνσης $d_k=-\nabla f(x_k)$ ορίζεται από την αρνητική κλίση της $f$ στο σημείο $x_k$.

    Ένα βασικό μειονέκτημα της μεθόδου Μέγιστης Καθόδου είναι πως η πορεία που "χαράζει" προς το ελάχιστο είναι μια τεθλασμένη γραμμή, όπου το διάνυσμα που ενώνει το $x_k$ με το $x_{k+1}$ είναι πάντα κάθετο στο διάνυσμα που ενώνει το $x_{k+1}$ με το $x_{k+2}$. Αυτή η συμπεριφορά καθιστά τη μέθοδο αυτή αργή ως προς τη σύγκλιση σε πολλές περιπτώσεις.
    \begin{algorithm}[H]
        \caption{Μέθοδος Μέγιστης Καθόδου}
        \begin{algorithmic}[1]
        \Require συνάρτηση $f$, αρχικό σημείο $x_1$, σταθερά $\epsilon>0$
        \Ensure προσέγγιση ελαχίστου $x^*$
        \State ορίστε $x_k \gets x$, $k \gets 1$
        \While{$|\nabla f(x_k)| > \epsilon$}
            \State $d_k \gets -\nabla f(x_k)$
            \State $\gamma_k \gets \arg\min_{\gamma>0} f(x_k + \gamma d_k)$
            \State $x_k \gets x_k + \gamma_k d_k$
            \State $k \gets k + 1$
        \EndWhile
        
        \State \Return $x^* \gets x_k$
        \end{algorithmic}
    \end{algorithm}
    
    \subsection{Γραφικές Παραστάσεις}

    Στον \textit{Πίνακα~\ref{tab:task2_results}} και στο \textit{Σχήμα~\ref{fig:task2_1}} παρουσιάζονται τα αποτελέσματα της μεθόδου Μέγιστης Καθόδου για τη συνάρτηση $f$. Καταγράφεται το ελάχιστο σημείο και οι επαναλήψεις για κάθε αρχικό σημείο και μέθοδο προσδιορισμού του βήματος.
    
    \begin{table}[H]
        \centering
        \footnotesize
        \begin{tabularx}{\linewidth}{|L|L|M|C|C|C|C|}
        \hline
            \thead{\textbf{Initial} \\ \textbf{Point}} &
            \thead{\textbf{Method}} &
            \thead{\textbf{Minimum} \\ \textbf{Point $x^*$}} &
            \thead{\textbf{Minimum} \\ \textbf{Value $f(x^*)$}} &
            \thead{\textbf{k}} &
            \thead{\textbf{Function} \\ \textbf{Evaluations}} &
            \thead{\textbf{Step} \\ \textbf{Evaluations}} \\
        \hline
            \csvreader[
                late after line=\\\hline
            ]{assets/task2_results.csv}{}
            {%
                \csvcoli & \csvcolii & \csvcoliii & \csvcoliv &
                \csvcolv & \csvcolvi & \csvcolvii
            }
        \end{tabularx}
        \caption{Συγκεντρωτικός πίνακας αποτελεσμάτων (Μέγιστη Κάθοδος)}
        \label{tab:task2_results}
    \end{table}

    \begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{assets/task2_1.jpg}
        \caption{Σύγκλιση της f ανά τις επαναλήψεις}
        \label{fig:task2_1}
    \end{figure}
    
    \subsection{Συμπεράσματα}

    Βάσει των παραπάνω αποτελεσμάτων παρατηρούμε ότι η συμπεριφορά του αλγορίθμου εξαρτάται έντονα τόσο από το αρχικό σημείο όσο και από τον τρόπο υπολογισμού του βήματος $\gamma_k$.

    \subsubsection*{Επίδραση Αρχικού Σημείου}
    
    \paragraph{Αρχικό σημείο $(0,0)$:} \
    Και οι τρεις μέθοδοι βήματος σταματούν αμέσως, καθώς στο σημείο αυτό ισχύει $\nabla f(0,0)=0$. Το σημείο όμως δεν είναι ελάχιστο, συνεπώς παρατηρείται εγκλωβισμός. Ο αλγόριθμος, βασιζόμενος αποκλειστικά στο μηδενικό gradient, θεωρεί λανθασμένα ότι έχει βρεθεί βέλτιστη λύση. Άρα, η μέθοδος δεν εγγυάται πάντα σύγκλιση.
    
    \paragraph{Αρχικό σημείο $(-1,-1)$:}
    Όλες οι παραλλαγές βήματος συγκλίνουν στο παγκόσμιο ελάχιστο $$x^* \approx (-1.22,\,-0.00848), \qquad f(x^*) \approx -0.41.$$ Ωστόσο, οι διαφορές στον αριθμό επαναλήψεων είναι μεγάλες. Το σταθερό βήμα και ο κανόνας Armijo απαιτούν περίπου $4.2\times 10^4$ επαναλήψεις, ενώ το βέλτιστο βήμα οδηγεί σε περίπου ένα δέκατο αυτού του πλήθους. Η αργή σύγκλιση οφείλεται στη χαρακτηριστική τεθλασμένη πορεία της μεθόδου.
    
    \paragraph{Αρχικό σημείο $(1,1)$:}
    Ο αλγόριθμος δεν οδηγείται στο πραγματικό ελάχιστο. Και οι τρεις μέθοδοι συγκλίνουν σε σημείο της μορφής $$x^* \approx (0.9,\;2), \qquad f(x^*) \approx 3\times10^{-8},$$ μια περιοχή όπου η $f$ μηδενίζεται εκθετικά. Η μέθοδος κατευθύνεται μακριά από το πραγματικό ελάχιστο, με αποτέλεσμα να κολλάει σε επίπεδη περιοχή. Ο μεγάλος αριθμός επαναλήψεων (έως και $3.4\times 10^5$) επιβεβαιώνει την δυσκολία πλοήγησης σε ανεπιθύμητες περιοχές του χώρου.
    
    \subsubsection*{Επίδραση του βήματος}
    
    \paragraph{Σταθερό Βήμα:}
    Απλό στην εφαρμογή, αλλά εξαιρετικά αργό. Σε στενές κοιλάδες προκαλεί μεγάλες ταλαντώσεις και χιλιάδες επαναλήψεις.
    
    \paragraph{Βέλτιστο βήμα:}
    Πετυχαίνει σημαντικά λιγότερες επαναλήψεις, αλλά απαιτεί πολύ μεγάλο αριθμό \textit{step evaluations}, λόγω της μονοδιάστατης αναζήτησης (μέθοδος Χρυσής Τομής). Έφερε τα καλύτερα αποτελέσματα για αυτή τη μέθοδο, παρ’ όλα αυτά, δεν εξασφαλίζει σύγκλιση στο σωστό ελάχιστο όταν το αρχικό σημείο είναι ακατάλληλο.
    
    \paragraph{Κανόνας Armijo:}
    Εξασφαλίζει μονοτονική μείωση της $f$, αλλά στη συγκεκριμένη συνάρτηση η συμπεριφορά του είναι σχεδόν ίδια με το σταθερό βήμα. Συχνά επιλέγει πολύ μικρά βήματα, οδηγώντας σε μεγάλο αριθμό επαναλήψεων.
    
    \subsubsection*{Γενικά Συμπεράσματα}
    
    \begin{itemize}
        \item Η μέθοδος Μέγιστης Καθόδου είναι ιδιαίτερα ευαίσθητη στα αρχικά σημεία. Δεν οδηγεί πάντα στο σωστό ελάχιστο.
        \item Το σταμάτημα στο $\nabla f(x_k)<\epsilon$ δεν αρκεί για να εγγυηθεί ελάχιστο (χαρακτηριστικό παράδειγμα το $(0,0)$).
        \item Η μορφή της συνάρτησης (στενές κοιλάδες, επίπεδες περιοχές) επηρεάζει δραματικά τη σύγκλιση.
        \item Το βήμα παίζει καθοριστικό ρόλο: μικρό βήμα σημαίνει πολύ αργή πρόοδο, μεγάλο βήμα μπορεί να οδηγήσει σε αστάθεια.
        \item Η χρήση βέλτιστου βήματος μειώνει τις επαναλήψεις, αλλά αυξάνει το συνολικό υπολογιστικό κόστος.
    \end{itemize}
    
    Συνολικά, η Μέθοδος Μέγιστης Καθόδου είναι αξιόπιστη μόνο όταν το πεδίο της συνάρτησης είναι «καλοήθες» και όταν το αρχικό σημείο βρίσκεται κοντά στην επιθυμητή περιοχή. Διαφορετικά, είτε οδηγείται σε λανθασμένα αποτελέσματα είτε παρουσιάζει εξαιρετικά αργή σύγκλιση.

\newpage
\section{Μέθοδος Newton}

    \subsection{Αλγόριθμος}
    
    Η μέθοδος Newton προκύπτει επιλέγοντας $\Delta_k=[\nabla^2 f(x_k)]^{-1}, \quad k=1,2,\dots$ και η επαναληπτική σχέση, λοιπόν, γίνεται: $$x_{k+1}=x_k-\gamma_k [\nabla^2f(x_k)]^{-1}\nabla f(x_k)$$
    Το διάνυσμα κατεύθυνσης, συνεπώς είναι: $d_k=-[\nabla^2f(x_k)]^{-1}\nabla f(x_k)$.

    Ένα βασικό μειονέκτημα της μεθόδου Newton είναι πως μεγάλα προβλήματα προκύπτουν όταν ο $\nabla^2 f(x)$ δεν είναι θετικά ορισμένος ή ακόμα και αντιστρέψιμος, πράγμα που συμβαίνει στην περίπτωση της συνάρτησης που εξετάζουμε σε αυτή την εργασία. Σε αυτές τις περιπτώσεις η μέθοδος Newton δεν ορίζεται ή δίνει λύσεις μακριά από το σημείο ολικού ελαχίστου.

    \begin{algorithm}[H]
        \caption{Μέθοδος Newton}
        \begin{algorithmic}[1]
        \Require συνάρτηση $f$, αρχικό σημείο $x_1$, σταθερά $\epsilon>0$
        \Ensure προσέγγιση ελαχίστου $x^*$
        \State ορίστε $x_k \gets x$, $k \gets 1$
        \While{$|\nabla f(x_k)| > \epsilon$}
            \State $d_k \gets -[\nabla^2 f(x_k)]^{-1} \nabla f(x_k)$
            \State $\gamma_k \gets \arg\min_{\gamma>0} f(x_k + \gamma d_k)$
            \State $x_k \gets x_k + \gamma_k d_k$
            \State $k \gets k + 1$
        \EndWhile
        
        \State \Return $x^* \gets x_k$
        \end{algorithmic}
    \end{algorithm}

    \subsection{Γραφικές Παραστάσεις}

    Στον \textit{Πίνακα~\ref{tab:task3_results}} και στο \textit{Σχήμα~\ref{fig:task3_1}} παρουσιάζονται τα αποτελέσματα της μεθόδου Μέγιστης Καθόδου για τη συνάρτηση $f$. Καταγράφεται το ελάχιστο σημείο και οι επαναλήψεις για κάθε αρχικό σημείο και μέθοδο προσδιορισμού του βήματος.

    \begin{table}[H]
        \centering
        \footnotesize
        \begin{tabularx}{\linewidth}{|L|L|M|C|C|C|C|}
        \hline
            \thead{\textbf{Initial} \\ \textbf{Point}} &
            \thead{\textbf{Method}} &
            \thead{\textbf{Minimum} \\ \textbf{Point $x^*$}} &
            \thead{\textbf{Minimum} \\ \textbf{Value $f(x^*)$}} &
            \thead{\textbf{k}} &
            \thead{\textbf{Function} \\ \textbf{Evaluations}} &
            \thead{\textbf{Step} \\ \textbf{Evaluations}} \\
        \hline
            \csvreader[
                late after line=\\\hline
            ]{assets/task3_results.csv}{}
            {%
                \csvcoli & \csvcolii & \csvcoliii & \csvcoliv &
                \csvcolv & \csvcolvi & \csvcolvii
            }
        \end{tabularx}
        \caption{Συγκεντρωτικός πίνακας αποτελεσμάτων (Newton)}
        \label{tab:task3_results}
    \end{table} 

    \begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{assets/task3_1.jpg}
        \caption{Σύγκλιση της f ανά τις επαναλήψεις}
        \label{fig:task3_1}
    \end{figure}

    \subsection{Συμπεράσματα}

    Τα αποτελέσματα δείχνουν ότι η μέθοδος Newton παρουσιάζει σημαντικές δυσκολίες στη συγκεκριμένη συνάρτηση. Ο βασικός λόγος είναι ότι ο $\nabla^2 f$ δεν είναι θετικά ορισμένος στα περισσότερα σημεία, με αποτέλεσμα ο υπολογισμός της κατεύθυνσης $$d_k = -[\nabla^2 f(x_k)]^{-1}\nabla f(x_k)$$ να οδηγεί σε βήματα που δεν κατευθύνονται προς ελάχιστο, αλλά προς περιοχές όπου η $f$ έχει επίπεδες ή ασταθείς καμπυλότητες.
    
    \subsubsection*{Επίδραση Αρχικού Σημείου / Βήματος}
    
    \paragraph{Αρχικό σημείο $(0,0)$:}
    Όπως και στη μέθοδο Μέγιστης Καθόδου, και οι τρεις μέθοδοι βήματος τερματίζουν αμέσως, επειδή $\nabla f(0,0)=0$. Το σημείο όμως δεν είναι ελάχιστο, συνεπώς παρατηρείται εγκλωβισμός.
    
    \paragraph{Αρχικό σημείο $(-1,-1)$:}
    Η συμπεριφορά διαφέρει έντονα ανάλογα με το βήμα:
    \begin{itemize}
        \item \textbf{Σταθερό βήμα:} Η μέθοδος συγκλίνει γρήγορα, αλλά όχι στο πραγματικό ελάχιστο. Το σημείο στο οποίο καταλήγει, $$x^* \approx (-0.376,\,-1.94), \qquad f(x^*)\approx -3.23\times 10^{-8},$$ είναι μια επίπεδη περιοχή όπου η $f$ είναι σχεδόν μηδενική, μακριά από τη βαθιά κοιλότητα του πραγματικού ελάχιστου.
        \item \textbf{Βέλτιστο βήμα:} Η μονοδιάστατη αναζήτηση οδηγεί σε χιλιάδες επαναλήψεις (3254) και τεράστιο αριθμό step evaluations ($\approx39k$), καταλήγοντας σε παρόμοιο σημείο με το σταθερό βήμα. Η μέθοδος δηλαδή «συμβιβάζεται» με μηδενικές τιμές της $f$ αντί να κινείται προς το αρνητικό ελάχιστο.
        \item \textbf{Κανόνας Armijo:} Η μέθοδος αποτυγχάνει πλήρως. Ο αλγόριθμος φτάνει το μέγιστο πλήθος επαναλήψεων χωρίς ουσιαστική μεταβολή: $$x_k = (-1,-1), \qquad f(x_k)=-0.135.$$ Ο λόγος είναι ότι ο μη θετικά ορισμένος Εσσιανός δίνει κατευθύνσεις που η μέθοδος Armijo απορρίπτει συνεχώς, με αποτέλεσμα ο αλγόριθμος να μην κινείται καθόλου, εώς ότου φτάσει στο όριο επαναλήψεων.
    \end{itemize}
    
    \paragraph{Αρχικό σημείο $(1,1)$:}
    Η συμπεριφορά είναι παρόμοια για κάθε βήμα:
    \begin{itemize}
        \item \textbf{Σταθερό βήμα:} Σύγκλιση σε σημείο της μορφής $$x^* \approx (0.376,\;1.94), \qquad f(x^*)\approx 3.23\times 10^{-8},$$ δηλαδή σε επίπεδη περιοχή μακριά από το παγκόσμιο ελάχιστο.
        \item \textbf{Βέλτιστο βήμα:} Μόνο εδώ η μέθοδος Newton συγκλίνει ταχύτατα (μόλις 9 επαναλήψεις). Ωστόσο, ακόμη και αυτή η σύγκλιση οδηγεί όχι στο πραγματικό ελάχιστο αλλά σε σημείο με σχεδόν μηδενική τιμή της $f$.
        \item \textbf{Armijo:} Σύγκλιση σε σημείο της μορφής $$x^* \approx (0.376,\;1.94), \qquad f(x^*)\approx 3.23\times 10^{-8},$$ δηλαδή σε επίπεδη περιοχή μακριά από το παγκόσμιο ελάχιστο.
    \end{itemize}
    
    \subsubsection*{Γενικά Συμπεράσματα}
    
    \begin{itemize}
        \item Η μέθοδος Newton δεν είναι κατάλληλη για συναρτήσεις με μη θετικά ορισμένο Εσσιανό.
        \item Η μέθοδος Newton λειτουργεί σωστά μόνο όταν το αρχικό σημείο βρίσκεται σε περιοχή όπου το Εσσιανό είναι θετικά ορισμένο. Εδώ αυτό δεν συμβαίνει.
        \item Παρά την ταχύτητα σύγκλισης σε ιδανικές περιπτώσεις, εδώ καταλήγει σε λανθασμένα σημεία.
        \item Η χρήση Armijo με Newton μπορεί να οδηγήσει σε πλήρη ακινητοποίηση του αλγορίθμου.
        \item Το βέλτιστο βήμα δεν διορθώνει το πρόβλημα του Εσσιανού, απλώς αυξάνει το κόστος.
        
    \end{itemize}
    
    Συνολικά, η μέθοδος Newton αποτυγχάνει να προσεγγίσει το παγκόσμιο ελάχιστο της $f$ και οδηγείται σε πλατώ. Αυτό την καθιστά σαφώς λιγότερο αξιόπιστη από τις υπόλοιπες μεθόδους για το συγκεκριμένο πρόβλημα.

\newpage
\section{Μέθοδος Levenberg-Marquardt}

    \subsection{Αλγόριθμος}

    Η μέθοδος Levenberg-Marquardt είναι μια τροποποίηση του αλγορίθμου Newton που αποσκοπεί στο να ξεπεράσει το πρόβλημα που προκύπτει αν ο $\nabla^2 f(x_k)$ δεν είναι θετικά ορισμένος. 
    Επιλέγουμε $\Delta_k=[\nabla^2 f(x_k)+\mu_kI]^{-1}, \quad k=1,2,\dots$, με $\mu_k$ τέτοιο ώστε ο $[\nabla^2 f(x_k)+\mu_kI]$ να είναι θετικά ορισμένος. Η επαναληπτική σχέση γίνεται: $$x_{k+1}=x_k-\gamma_k [\nabla^2f(x_k)+\mu_kI]^{-1}\nabla f(x_k)$$
    Το διάνυσμα κατεύθυνσης, συνεπώς είναι: $d_k=-[\nabla^2f(x_k)+\mu_kI]^{-1}\nabla f(x_k)$.

    Η μέθοδος αυτή αποτελεί τον συγκερασμό της μεθόδου Newton και Μέγιστης Καθόδου. Όταν το $\mu_k$ είναι αρκετά μεγάλο, ο παράγοντας $\mu_kI$ κυριαρχεί σε σχέση με τον $\nabla^2f(x_k)$. Κατά συνέπεια η μέθοδος Levenberg-Marquardt θα λειτουργεί σχεδόν όπως η μέθοδος της Μέγιστης Καθόδου. Αν όμως το $\mu_k$ είναι αρκετά μικρό, τότε ο $\nabla^2f(x_k)$ υπερισχύει και η μέθοδος συμπεριφέρεται σαν τη μέθοδο Newton.
    
    \begin{algorithm}[H]
        \caption{Μέθοδος Levenberg-Marquardt}
        \begin{algorithmic}[1]
        \Require συνάρτηση $f$, αρχικό σημείο $x_1$, σταθερά $\epsilon>0$
        \Ensure προσέγγιση ελαχίστου $x^*$
        \State ορίστε $x_k \gets x$, $k \gets 1$
        \While{$|\nabla f(x_k)| > \epsilon$}
            \State ορίστε $\mu_k$ τέτοιο ώστε ο $[\nabla^2 f(x_k)+\mu_kI]$ θετικά ορισμένος
            \State $d_k \gets -[\nabla^2 f(x_k)+\mu_kI]^{-1} \nabla f(x_k)$
            \State ορίστε $\gamma_k$ τέτοιο ώστε να ικανοποιούνται τα κριτήρια 3, 4 \cite{book}
            \State $x_k \gets x_k + \gamma_k d_k$
            \State $k \gets k + 1$
        \EndWhile
        
        \State \Return $x^* \gets x_k$
        \end{algorithmic}
    \end{algorithm}

    \subsection{Γραφικές Παραστάσεις}

    Στον \textit{Πίνακα~\ref{tab:task4_results}} και στο \textit{Σχήμα~\ref{fig:task4_1}} παρουσιάζονται τα αποτελέσματα της μεθόδου Μέγιστης Καθόδου για τη συνάρτηση $f$. Καταγράφεται το ελάχιστο σημείο και οι επαναλήψεις για κάθε αρχικό σημείο και μέθοδο προσδιορισμού του βήματος.

    \begin{table}[H]
        \centering
        \footnotesize
        \begin{tabularx}{\linewidth}{|L|L|M|C|C|C|C|}
        \hline
            \thead{\textbf{Initial} \\ \textbf{Point}} &
            \thead{\textbf{Method}} &
            \thead{\textbf{Minimum} \\ \textbf{Point $x^*$}} &
            \thead{\textbf{Minimum} \\ \textbf{Value $f(x^*)$}} &
            \thead{\textbf{k}} &
            \thead{\textbf{Function} \\ \textbf{Evaluations}} &
            \thead{\textbf{Step} \\ \textbf{Evaluations}} \\
        \hline
            \csvreader[
                late after line=\\\hline
            ]{assets/task4_results.csv}{}
            {%
                \csvcoli & \csvcolii & \csvcoliii & \csvcoliv &
                \csvcolv & \csvcolvi & \csvcolvii
            }
        \end{tabularx}
        \caption{Συγκεντρωτικός πίνακας αποτελεσμάτων (Levenberg-Marquardt)}
        \label{tab:task4_results}
    \end{table}

    \begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{assets/task4_1.jpg}
        \caption{Σύγκλιση της f ανά τις επαναλήψεις}
        \label{fig:task4_1}
    \end{figure}
    
    \subsection{Συμπεράσματα}

    Η μέθοδος Levenberg-Marquardt συνδυάζει τα χαρακτηριστικά της μεθόδου Μέγιστης Καθόδου και Newton. Σε σημεία όπου ο Εσσιανός δεν είναι θετικά ορισμένος, ο όρος απόσβεσης~$\mu_kI$ σταθεροποιεί την κατεύθυνση και επιτρέπει στον αλγόριθμο να συνεχίσει, αποφεύγοντας τις αποτυχίες της μεθόδου Newton. Τα αποτελέσματα επιβεβαιώνουν ότι η μέθοδος είναι πιο ανθεκτική και σταθερή, χωρίς όμως να εξαλείφει εντελώς τα προβλήματα που σχετίζονται με τη μορφή της συνάρτησης.
    
    \subsubsection*{Επίδραση Αρχικού Σημείου}
    
    \paragraph{Αρχικό σημείο $(0,0)$.}
    Όπως και στις προηγούμενες μεθόδους, η διαδικασία τερματίζει αμέσως επειδή στο σημείο αυτό ισχύει $\nabla f(0,0)=0$. Το σημείο όμως δεν είναι ελάχιστο.
    
    \paragraph{Αρχικό σημείο $(-1,-1)$.}
    Όλες οι παραλλαγές βήματος συγκλίνουν στο ίδιο σημείο: $$x^* \approx (-1.22,\,-0.008), \qquad f(x^*) \approx -0.41,$$ που αποτελεί το παγκόσμιο ελάχιστο της συνάρτησης.
    
    \paragraph{Αρχικό σημείο $(1,1)$.}
    Η μέθοδος συγκλίνει στο $$x^* \approx (1.22,\;2.0\text{--}2.03), \qquad f(x^*) \approx 2\times10^{-8}.$$ Πρόκειται για επίπεδη περιοχή όπου η $f$ σχεδόν μηδενίζεται, αλλά δεν αποτελεί το αρνητικό παγκόσμιο ελάχιστο της συνάρτησης. Η μέθοδος Levenberg-Marquardt, όπως και η μέθοδος Μέγιστης Καθόδου, ακολουθεί την τοπική γεωμετρία της συνάρτησης και οδηγείται σε αυτήν την επίπεδη περιοχή αντί να κινηθεί προς την κοιλότητα γύρω από $(-1.22,-0.008)$.

    \subsubsection*{Επίδραση Βήματος}
    \begin{itemize}
        \item \textbf{Σταθερό βήμα:} Πραγματοποιεί 131 επαναλήψεις. Η σύγκλιση είναι σταθερή, χωρίς ταλαντώσεις, αλλά σχετικά αργή.
        \item \textbf{Βέλτιστο βήμα:} Είναι ξεκάθαρα η πιο αποδοτική παραλλαγή. Χρειάζεται μόλις 14 επαναλήψεις και περιορισμένο πλήθος function evaluations. Η μέθοδος επωφελείται ιδιαίτερα από την εύρεση του ιδανικού μεγέθους βήματος σε κάθε επανάληψη.
        \item \textbf{Κανόνας Armijo:} Έχει ίδια συμπεριφορά με το σταθερό βήμα, με 131 επαναλήψεις και παρόμοια ακρίβεια. Ο Armijo τείνει να επιλέγει σχετικά μικρά βήματα, οπότε η σύγκλιση είναι σταθερή αλλά όχι γρήγορη.
    \end{itemize}
    
    \subsubsection*{Γενικά Συμπεράσματα}
    
    \begin{itemize}
        \item Η μέθοδος Levenberg-Marquardt είναι πολύ πιο σταθερή από τη μέθοδο Newton, επειδή ο όρος απόσβεσης~$\mu_k$ διορθώνει τα προβλήματα μη θετικά ορισμένου Εσσιανού.
        \item Παρά τη σταθερότητα, η μέθοδος παραμένει ευαίσθητη στο σημείο εκκίνησης. Από το $(1,1)$ δεν προσεγγίζει το παγκόσμιο ελάχιστο, αλλά μια επίπεδη περιοχή με σχεδόν μηδενική τιμή της $f$.
        \item Το βέλτιστο βήμα είναι μακράν η πιο αποδοτική παραλλαγή: μειώνει τις επαναλήψεις κατά έναν παράγοντα 10 σε σχέση με τις άλλες επιλογές.
        \item Ο Armijo έχει συμπεριφορά σχεδόν ταυτόσημη με το σταθερό βήμα, επιλέγει συνήθως μικρά βήματα, κάτι που οδηγεί σε αργή, αλλά ασφαλή σύγκλιση.
    \end{itemize}
    
    Συνολικά, η μέθοδος Levenberg-Marquardt αποδεικνύεται η πιο σταθερή και αξιόπιστη από όλες τις μεθόδους που εφαρμόστηκαν, χωρίς όμως να εξασφαλίζει σύγκλιση στο παγκόσμιο ελάχιστο όταν το αρχικό σημείο βρίσκεται σε περιοχή όπου η συνάρτηση εμφανίζει επίπεδες γεωμετρίες.

\newpage
\section{Σύγκριση Μεθόδων}

    Σε αυτή την ενότητα θα χρησιμοποιήσουμε τα ευρήματα από τα όλα τα θέματα για να συγκρίνουμε τις μεθόδους μεταξύ τους ως προς την ορθότητα και την αποδοτικότητα.
    \begin{itemize}
        \item \textbf{Παρατήρηση 1:}  
        Η επιλογή αρχικού σημείου επηρεάζει καθοριστικά και τις τρεις μεθόδους. Στην περιοχή του $(0,0)$ όλες οι μέθοδοι εγκλωβίζονται σε σημείο όπου το gradient μηδενίζεται χωρίς να υπάρχει ελάχιστο.
    
        \item \textbf{Παρατήρηση 2:}  
        Από το $(-1,-1)$ τόσο η Μέγιστη Κάθοδος όσο και η Levenberg--Marquardt συγκλίνουν στο πραγματικό ελάχιστο. Η μέθοδος Newton αποτυγχάνει λόγω μη θετικά ορισμένου Εσσιανού.
    
        \item \textbf{Παρατήρηση 3:}  
        Από το $(1,1)$ όλες οι μέθοδοι εκτός από τη Newton καταλήγουν σε επίπεδη περιοχή με σχεδόν μηδενική τιμή της $f$, αλλά όχι στο παγκόσμιο ελάχιστο. Η Newton συγκλίνει γρήγορα αλλά επίσης καταλήγει λανθασμένα.
    
        \item \textbf{Παρατήρηση 4:}  
        Το βέλτιστο βήμα μειώνει δραστικά τις επαναλήψεις σε όλες τις μεθόδους, αλλά αυξάνει τον αριθμό αξιολογήσεων λόγω της μονοδιάστατης αναζήτησης.
    
        \item \textbf{Παρατήρηση 5:}  
        Ο Armijo είναι σταθερός αλλά συντηρητικός: συνήθως επιλέγει μικρά βήματα και οδηγεί σε αργή αλλά σταθερή σύγκλιση. Στη Newton μπορεί να προκαλέσει πλήρη στασιμότητα.
    \end{itemize}

    Ακολουθεί πίνακας που συνοψίζει τα ευρήματα και τα συμπεράσματα της εργασίας αυτής:
    \begin{table}[H]
        \centering
        \small
        \begin{tabularx}{\linewidth}{|p{3cm}|X|X|p{3cm}|}
            \hline
            \textbf{Μέθοδος} &
            \textbf{Πλεονεκτήματα} &
            \textbf{Μειονεκτήματα} &
            \textbf{Χρήση} \\
            \hline
    
            \makecell[L]{Μέγιστη \\Κάθοδος} &
            Πάντα δίνει κατεύθυνση καθόδου, απλή υλοποίηση, δεν απαιτεί Εσσιανό. &
            Πολύ αργή σύγκλιση, ευαισθησία στο βήμα, τεθλασμένη πορεία και ταλαντώσεις, εύκολος εγκλωβισμός σε επίπεδες περιοχές. &
            Όταν δεν υπάρχει Εσσιανός ή η συνάρτηση είναι απλή. \\
            \hline
    
            \makecell[L]{Newton} &
            Πολύ γρήγορη σύγκλιση όταν $\nabla^2 f>0$, τετραγωνική σύγκλιση κοντά στο ελάχιστο. &
            Αποτυγχάνει όταν $\nabla^2 f<0$, μπορεί να απομακρυνθεί από το ελάχιστο, απαιτεί υπολογισμό και αντιστροφή Εσσιανού. &
            Όταν ο Εσσιανός είναι θετικά ορισμένος ή γενικά καλά συμπεριφερόμενο. \\
            \hline
    
            \makecell[L]{Levenberg-\\Marquardt} &
            Πιο σταθερή από τη Newton, αντέχει μη θετικά ορισμένο Εσσιανό, έχει πολύ καλή αποδοτικότητα. &
            Μπορεί να συμπεριφερθεί σαν Steepest Descend και να είναι αργή, δεν εγγυάται σύγκλιση στο παγκόσμιο ελάχιστο, απαιτεί προσεκτική επιλογή του $\mu_k$. &
            Για προβλήματα όπου η Newton είναι ασταθής. \\
            \hline
        \end{tabularx}
    
        \caption{Συνοπτική σύγκριση των μεθόδων βελτιστοποίησης}
    \end{table}
    
\appendix

\section{Εργαλεία}
Τα εργαλεία που χρησιμοποιήθηκαν κατά την εκπόνηση της εργασίας είναι τα εξής:
\begin{itemize}
    \item MATLAB
    \item LaTeX
    \item Git
\end{itemize}

\section{Βιβλιογραφία}
\begin{thebibliography}{9}
    \bibitem[\href{https://www.tziola.gr/book/rovi/}{1}]{book}
         Γ. Ροβιθάκης, \textit{Τεχνικές Βελτιστοποίησης}, Εκδόσεις Τζιόλα, 2007.  
\end{thebibliography}

\end{document}