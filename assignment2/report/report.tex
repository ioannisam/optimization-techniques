\documentclass[a4paper,12pt]{article}

\usepackage{fontspec}
\usepackage{microtype}
\usepackage{polyglossia}

\setmainlanguage{greek}
\setotherlanguage{english}

\setmainfont{Latin Modern Roman}
\newfontfamily\greekfont{CMU Serif}[Script=Greek]

\usepackage[a4paper, top=1.5cm, bottom=1.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{array}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=black,
    urlcolor=blue
}
\usepackage[super,sort&compress]{cite}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{tabularx}
\usepackage{makecell}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\newcolumntype{C}{>{\centering\arraybackslash}c}

\title{Τεχνικές Βελτιστοποίησης: Εργασία 2}
\author{Ιωάννης Μιχάλαινας ΑΕΜ:10902}
\date{Νοέμβριος 2025}

\begin{document}

\maketitle

\begin{center}
\textbf{Αποθετήριο Κώδικα:} \\
\href{https://github.com/ioannisam/optimization-techniques}
     {github.com/ioannisam/optimization-techniques}
\end{center}

\bigskip

\begin{abstract}
    Στην εργασία αυτή υλοποιούνται και συγκρίνονται n-διάστατες μέθοδοι βελτιστοποίησης - συγκεκριμένα η μέθοδος Μέγιστης Καθόδου, η μέθοδος Newton και η μέθοδος Levenberg-Marquardt. Για καθεμία από αυτές εξετάζεται ο αριθμός υπολογισμών και η σύγκλιση, ως προς την ταχύτητα και την ακρίβεια, εφαρμόζοντάς τες από διαφορετικά αρχικά σημεία και με διαφορετικές μεθόδους υπολογισμού του βήματος. Τα αποτελέσματα δείχνουν τις ομοιότητες και τις διαφορές μεταξύ των μεθόδων και αναδεικνύουν τα πλεονεκτήματα και τους περιορισμούς της καθεμιάς. Ο σκοπός της εργασίας αυτής είναι η εξοικείωση με τις παραπάνω μεθόδους και η σύγκριση μεταξύ τους, όσον αφορά την αποδοτικότητα.
\end{abstract}

\tableofcontents
\clearpage

\section{Εισαγωγή}

Στην εργασία αυτή μελετάμε διάφορες τεχνικές βελτιστοποίησης, ονομαστικά τη μέθοδο Μέγιστης Καθόδου, τη μέθοδο Newton και τη μέθοδο Levenberg-Marquardt. Ο σκοπός είναι η προσέγγιση του $x^*$, της λύσης του προβλήματος ελαχιστοποίησης.

Οι μέθοδοι που θα εξετάσουμε βασίζονται στην ιδέα της επαναληπτικής καθόδου, η οποία οδηγεί σε ολοένα και βελτιωμένες τιμές της $f$. $$f(x_{k+1}) < f(x_k), \quad k=0,1,2,\dots$$

Στόχος των μεθόδων αυτών είναι με κάθε επανάληψη συγκλίνουμε προς το $x^*$. Αυτό επιτυγχάνεται μέσω της επαναληπτικής σχέσης: $$x_{k+1}=x_k+\gamma_kd_k$$ Όπου $k$ ο αριθμός των επαναλήψεων, $d_k$ το διάνυσμα της κατεύθυνσης του αλγορίθμου που ικανοποιεί τη συνθήκη $\nabla f^\top(x_k)d_k<0$ και $\gamma_k>0$, τέτοιο ώστε να ισχύει η συνθήκη $f(x_k+\gamma_kd_k)<f(x_k)$.

Οι αλγόριθμοι που θα μελετήσουμε είναι της γενικής μορφής $$x_{k+1}=x_k-\gamma_k\Delta_k \nabla f(x_k)$$

Η επιλογή του βήματος $\gamma_k$ είναι νευραλγικής σημασίας για την ορθότητα του αλγορίθμου, αφού αν επιλεγεί πολύ μικρό θα οδηγήσει σε ασήμαντη μείωση της $f$ και κατά συνέπεια αργή σύγκλιση. Αντίθετα, αν επιλεγεί πολύ μεγάλο μπορεί να οδηγήσει τον αλγόριθμο σε αστάθεια. Θα εξετάσουμε τρεις διαφορετικούς τρόπους προσδιορισμού του βήματος:
\begin{itemize}
    \item \textbf{Σταθερό Βήμα:} Κρατάμε σταθερό το βήμα $\gamma_k$ καθ' όλη τη διάρκεια της αναζήτησης. Αποτελεί την απλούστερη μέθοδο, αφού ελαχιστοποιεί την υπολογιστική πολυπλοκότητα του αλγορίθμου. Δεδομένης της ευαισθησίας του ζητήματος της επιλογής βήματος, αυτή η μέθοδος προϋποθέτει μεγάλη εμπειρεία και ακρίβεια για την ορθή εφαρμογή της.
    \item \textbf{Βέλτιστο Βήμα:} Επιλέγουμε βήμα τέτοιο ώστε να ελαχιστοποιηθεί η $f(x_k+\gamma_k d_k)$ ως προς $\gamma_k$. Αυτό αποτελεί μονοδιάστατο πρόβλημα βελτιστοποίησης που σε αυτή την εργασία θα επιλύεται με τη βοήθεια της μεθόδου Χρυσής Τομής, υλοποιημένης στην προηγούμενη εργασία. Επιλέγουμε τη μέθοδο Χρυσής Τομής διότι είναι η ταχύτερη μέθοδος χωρίς χρήση παραγώγων που δεν απαιτεί τον εκ των προτέρων υπολογισμό των συνολικών επαναλήψεων.
    \item \textbf{Κανόνας Armijo:} Σύμφωνα με τον κανόνα Armijo, το βήμα επιλέγεται ως $\gamma_k=s\beta^{\mu_k}$, όπου $\mu_k \in \mathbb{Z}_{\ge 0}$ τέτοιο ώστε $f(x_k)-f(x_{k+1}) \ge -\alpha \beta^{\mu_k}sd_k^{\top} \nabla f(x_k)$. Εδώ ο συντελεστής s δηλώνει το αρχικό βήμα. Συνήθως ισχύει $\alpha \in [10^{-5}, 10^{-1}]$ και $\beta \in [{1\over10}, {1\over2}]$.
\end{itemize}


Δοσμένων των αρχικών σημείων $[0,0]$, $[-1,-1]$ και $[1,1]$ θα ελαχιστοποιηθεί η: $$f(x,y) = x^3e^{-x^2-y^4}$$

Η συνάρτηση $f$ είναι δύο φορές παραγωγίσιμη, συνθήκη απαραίτητη για την εφαρμογή των αλγορίθμων που θα εξετάσουμε. Παρακάτω παρουσιάζεται η γραφική της παράσταση τόσο στον τρισδιάστατο χώρο όσο και στον δισδιάστατο χώρο, υπό τη μορφή ισοβαρών καμπυλών. Αυτή φαίνεται να παρουσιάζει ελάχιστο στο $[-1.225, 0.009]$ με τιμή $f_{min}=-0.41$. Για θετικές τιμές του $x$, η συνάρτηση αποκτά θετικές τιμές και σχηματίζει έναν ομαλό λόφο, ενώ για αρνητικές τιμές του $x$ εμφανίζεται μια βαθιά κοιλότητα η οποία οδηγεί στο τοπικό ελάχιστο που εντοπίστηκε.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/task1_1.jpg}
    \caption{Τρισδιάστατη απεικόνιση της συνάρτησης f}
    \label{fig:task1_1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{assets/task1_2.jpg}
    \caption{Ισοεπίπεδη απεικόνιση της συνάρτησης f}
    \label{fig:task1_2}
\end{figure}

\newpage
\section{Μέθοδος Μέγιστης Καθόδου}

    \subsection{Αλγόριθμος}

    Η μέθοδος της Μέγιστης Καθόδου προκύπτει επιλέγοντας $\Delta_k=I \quad k=1,2,\dots$ και η επαναληπτική σχέση, λοιπόν, γίνεται: $$x_{k+1}=x_k-\gamma_k \nabla f(x_k)$$
    Το διάνυσμα κατεύθυνσης $d_k=-\nabla f(x_k)$ ορίζεται από την αρνητική κλίση της $f$ στο σημείο $x_k$.

    Ένα βασικό μειονέκτημα της μεθόδου Μέγιστης Καθόδου είναι πως η πορεία που "χαράζει" προς το ελάχιστο είναι μια τεθλασμένη γραμμή, όπου το διάνυσμα που ενώνει το $x_k$ με το $x_{k+1}$ είναι πάντα κάθετο στο διάνυσμα που ενώνει το $x_{k+1}$ με το $x_{k+2}$. Αυτή η συμπεριφορά καθιστά τη μέθοδο αυτή αργή ως προς τη σύγκλιση σε πολλές περιπτώσεις.
    \begin{algorithm}[H]
        \caption{Μέθοδος Μέγιστης Καθόδου}
        \begin{algorithmic}[1]
        \Require συνάρτηση $f$, αρχικό σημείο $x_1$, σταθερά $\epsilon>0$
        \Ensure προσέγγιση ελαχίστου $x^*$
        \State ορίστε $x_k \gets x$, $k \gets 1$
        \While{$|\nabla f(x_k)| > \epsilon$}
            \State $d_k \gets -\nabla f(x_k)$
            \State $\gamma_k \gets \arg\min_{\gamma>0} f(x_k + \gamma d_k)$
            \State $x_k \gets x_k + \gamma_k d_k$
            \State $k \gets k + 1$
        \EndWhile
        
        \State \Return $x^* \gets x_k$
        \end{algorithmic}
    \end{algorithm}
    
    \subsection{Γραφικές Παραστάσεις}

    Στο \textit{Σχήμα~\ref{fig:task2_1}} παρουσιάζεται ...
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{assets/task2_1.jpg}
        \caption{Σύγκλιση της f ανά τις επαναλήψεις}
        \label{fig:task2_1}
    \end{figure}
    
    \subsection{Συμπεράσματα}

\newpage
\section{Μέθοδος Newton}

    \subsection{Αλγόριθμος}
    
    Η μέθοδος Newton προκύπτει επιλέγοντας $\Delta_k=[\nabla^2 f(x_k)]^{-1} \quad k=1,2,\dots$ και η επαναληπτική σχέση, λοιπόν, γίνεται: $$x_{k+1}=x_k-\gamma_k [\nabla^2f(x_k)]^{-1}\nabla f(x_k)$$
    Το διάνυσμα κατεύθυνσης, συνεπώς είναι: $d_k=-[\nabla^2f(x_k)]^{-1}\nabla f(x_k)$.

    Ένα βασικό μειονέκτημα της μεθόδου Newton είναι πως μεγάλα προβλήματα προκύπτουν όταν ο $\nabla^2 f(x)$ δεν είναι θετικά ορισμένος ή ακόμα και αντιστρέψιμος, πράγμα που συμβαίνει στην περίπτωση της συνάρτησης που εξετάζουμε σε αυτή την εργασία. Σε αυτές τις περιπτώσεις η μέθοδος Newton δεν ορίζεται ή δίνει λύσεις μακριά από το σημείο ολικού ελαχίστου.

    \begin{algorithm}[H]
        \caption{Μέθοδος Newton}
        \begin{algorithmic}[1]
        \Require συνάρτηση $f$, αρχικό σημείο $x_1$, σταθερά $\epsilon>0$
        \Ensure προσέγγιση ελαχίστου $x^*$
        \State ορίστε $x_k \gets x$, $k \gets 1$
        \While{$|\nabla f(x_k)| > \epsilon$}
            \State $d_k \gets -[\nabla^2 f(x_k)]^{-1} \nabla f(x_k)$
            \State $\gamma_k \gets \arg\min_{\gamma>0} f(x_k + \gamma d_k)$
            \State $x_k \gets x_k + \gamma_k d_k$
            \State $k \gets k + 1$
        \EndWhile
        
        \State \Return $x^* \gets x_k$
        \end{algorithmic}
    \end{algorithm}

    \subsection{Γραφικές Παραστάσεις}

    Στο \textit{Σχήμα~\ref{fig:task3_1}} παρουσιάζεται ...  

    \begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{assets/task3_1.jpg}
        \caption{Σύγκλιση της f ανά τις επαναλήψεις}
        \label{fig:task3_1}
    \end{figure}

    \subsection{Συμπεράσματα}

\newpage
\section{Μέθοδος Levenberg-Marquardt}

    \subsection{Αλγόριθμος}

    Η μέθοδος Levenberg-Marquardt είναι μια τροποποίηση του αλγορίθμου Newton που αποσκοπεί στο να ξεπεράσει το πρόβλημα που προκύπτει αν ο $\nabla^2 f(x_k)$ δεν είναι θετικά ορισμένος. 
    Επιλέγουμε $\Delta_k=[\nabla^2 f(x_k)+\mu_kI]^{-1} \quad k=1,2,\dots$, με $\mu_k$ τέτοιο ώστε ο $[\nabla^2 f(x_k)+\mu_kI]$ να είναι θετικά ορισμένος. Η επαναληπτική σχέση γίνεται: $$x_{k+1}=x_k-\gamma_k [\nabla^2f(x_k)+\mu_kI]^{-1}\nabla f(x_k)$$
    Το διάνυσμα κατεύθυνσης, συνεπώς είναι: $d_k=-[\nabla^2f(x_k)+\mu_kI]^{-1}\nabla f(x_k)$.

    Η μέθοδος αυτή αποτελεί τον συγκερασμό της μεθόδου Newton και Μέγιστης Καθόδου. Όταν το $\mu_k$ είναι αρκετά μεγάλο, ο παράγοντας $\mu_kI$ κυριαρχεί σε σχέση με τον $\nabla^2f(x_k)$. Κατά συνέπεια η μέθοδος Levenberg-Marquardt θα λειτουργεί σχεδόν όπως η μέθοδος της Μέγιστης Καθόδου. Αν όμως το $\mu_k$ είναι αρκετά μικρό, τότε ο $\nabla^2f(x_k)$ υπερισχύει και η μέθοδος συμπεριφέρεται σαν τη μέθοδο Newton.
    
    \begin{algorithm}[H]
        \caption{Μέθοδος Levenberg-Marquardt}
        \begin{algorithmic}[1]
        \Require συνάρτηση $f$, αρχικό σημείο $x_1$, σταθερά $\epsilon>0$
        \Ensure προσέγγιση ελαχίστου $x^*$
        \State ορίστε $x_k \gets x$, $k \gets 1$
        \While{$|\nabla f(x_k)| > \epsilon$}
            \State ορίστε $\mu_k$ τέτοιο ώστε ο $[\nabla^2 f(x_k)+\mu_kI]$ θετικά ορισμένος
            \State $d_k \gets -[\nabla^2 f(x_k)+\mu_kI]^{-1} \nabla f(x_k)$
            \State ορίστε $\gamma_k$ τέτοιο ώστε να ικανοποιούνται τα κριτήρια 3, 4 \cite{book}
            \State $x_k \gets x_k + \gamma_k d_k$
            \State $k \gets k + 1$
        \EndWhile
        
        \State \Return $x^* \gets x_k$
        \end{algorithmic}
    \end{algorithm}

    \subsection{Γραφικές Παραστάσεις}

    Στο \textit{Σχήμα~\ref{fig:task4_1}} παρουσιάζεται ...
    \begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{assets/task4_1.jpg}
        \caption{Σύγκλιση της f ανά τις επαναλήψεις}
        \label{fig:task4_1}
    \end{figure}
    
    \subsection{Συμπεράσματα}

\newpage
\section{Σύγκριση Μεθόδων}

    Σε αυτή την ενότητα θα χρησιμοποιήσουμε τα ευρήματα από τα όλα τα θέματα για να συγκρίνουμε τις μεθόδους μεταξύ τους ως προς την αποδοτικότητα.

    \begin{itemize}
        \item \textbf{Παρατήρηση 1:} 
    
        \item \textbf{Παρατήρηση 2:}
        
        \item \textbf{Παρατήρηση 3:}
        
        \item \textbf{Παρατήρηση 4:} 
        
        \item \textbf{Παρατήρηση 5:} 
    \end{itemize}

    Ακολουθεί πίνακας που συνοψίζει τα ευρήματα και τα συμπεράσματα της εργασίας αυτής:
    \begin{table}[H]
        \centering
        \small
        \begin{tabularx}{\linewidth}{|p{3cm}|X|X|p{3cm}|}
            \hline
            \textbf{Μέθοδος} &
            \textbf{Πλεονεκτήματα} &
            \textbf{Μειονεκτήματα} &
            \textbf{Χρήση} \\
            \hline
    
            \makecell[l]{Μέγιστης\\Καθόδου} &
            ... &
            ... &
            ... \\
            \hline
    
            \makecell[l]{Newton} &
            ... &
            ... &
            ... \\
            \hline
    
            \makecell[l]{Levenberg-\\Marquardt} &
            ... &
            ... &
            ... \\
            \hline
        \end{tabularx}
    
        \caption{Σύγκριση μεθόδων βελτιστοποίησης: πλεονεκτήματα, μειονεκτήματα και συνθήκες χρήσης.}
        \label{tab:method-comparison}
    \end{table}
    
\appendix

\section{Εργαλεία}
Τα εργαλεία που χρησιμοποιήθηκαν κατά την εκπόνηση της εργασίας είναι τα εξής:
\begin{itemize}
    \item MATLAB
    \item LaTeX
    \item Git
\end{itemize}

\section{Βιβλιογραφία}
\begin{thebibliography}{9}
    \bibitem[\href{https://www.tziola.gr/book/rovi/}{1}]{book}
         Γ. Ροβιθάκης, \textit{Τεχνικές Βελτιστοποίησης}, Εκδόσεις Τζιόλα, 2007.  
\end{thebibliography}

\end{document}